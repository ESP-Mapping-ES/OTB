\chapter{Classification}

\section{Machine Learning Framework}

\subsection{Machine learning models}
\label{sec:MLGenericFramework}

The OTB classification is implemented as a generic Machine Learning
framework, supporting several possible machine learning libraries as backends.
The base class \doxygen{otb}{MachineLearningModel} defines this framework.
As of now libSVM (the machine learning library historically integrated in OTB),
machine learning methods of OpenCV library (\cite{opencv_library}) and also
Shark machine learning library (\cite{shark_library}) are available. Both
supervised and unsupervised classifiers are supported in the framework.

The current list of classifiers available through the same generic interface within the OTB is:

\begin{itemize}
  \item \textbf{LibSVM}: Support Vector Machines classifier based on libSVM.
  \item \textbf{SVM}: Support Vector Machines classifier based on OpenCV, itself based on libSVM.
  \item \textbf{Bayes}: Normal Bayes classifier based on OpenCV.
  \item \textbf{Boost}: Boost classifier based on OpenCV.
  \item \textbf{DT}: Decision Tree classifier based on OpenCV.
  \item \textbf{RF}: Random Forests classifier based on the Random Trees in OpenCV.
  \item \textbf{KNN}: K-Nearest Neighbors classifier based on OpenCV.
  \item \textbf{ANN}: Artificial Neural Network classifier based on OpenCV.
  \item \textbf{SharkRF} : Random Forests classifier based on Shark.
  \item \textbf{SharkKM} : KMeans unsupervised classifier based on Shark.
\end{itemize}

These models have a common interface, with the following major functions:
\begin{itemize}
  \item \code{SetInputListSample(InputListSampleType *in)} : set the list of input samples
  \item \code{SetTargetListSample(TargetListSampleType *in)} : set the list of target samples
  \item \code{Train()} : train the model based on input samples
  \item \code{Save(...)} : saves the model to file
  \item \code{Load(...)} : load a model from file
  \item \code{Predict(...)} : predict a target value for an input sample
  \item \code{PredictBatch(...)} : prediction on a list of input samples
\end{itemize}

The \code{PredictBatch(...)} function can be multi-threaded when
called either from a multi-threaded filter, or from a single location. In
the later case, it creates several threads using OpenMP.
There is a factory mechanism on top of the model class (see
\doxygen{otb}{MachineLearningModelFactory}). Given an input file,
the static function \code{CreateMachineLearningModel(...)} is able
to instantiate a model of the right type.

For unsupervised models, the target samples \textbf{still have to be set}. They
won't be used so you can fill a ListSample with zeros.

%-------------------------------------------------------------------------------
\subsection{Training a model}

The models are trained from a list of input samples, stored in a
\subdoxygen{itk}{Statistics}{ListSample}. For supervised classifiers, they
also need a list of targets associated to each input sample. Whatever the
source of samples, it has to be converted into a \code{ListSample} before
being fed into the model.

Then, model-specific parameters can be set. And finally, the \code{Train()}
method starts the learning step. Once the model is trained it can be saved
to file using the function \code{Save()}. The following examples show how
to do that.

\input{TrainMachineLearningModelFromSamplesExample.tex}

\input{TrainMachineLearningModelFromImagesExample.tex}

%-------------------------------------------------------------------------------
\subsection{Prediction of a model}

For the prediction step, the usual process is to:
\begin{itemize}
\item Load an existing model from a file.
\item Convert the data to predict into a \code{ListSample}.
\item Run the \code{PredictBatch(...)} function.
\end{itemize}

There is an image filter that perform this step on a whole image, supporting
streaming and multi-threading: \doxygen{otb}{ImageClassificationFilter}.

\ifitkFullVersion
\input{SupervisedImageClassificationExample.tex}
\fi

%-------------------------------------------------------------------------------
\subsection{Integration in applications}

The classifiers are integrated in several OTB Applications. There is a base
class that provides an easy access to all the classifiers:
\subdoxygen{otb}{Wrapper}{LearningApplicationBase}. As each machine learning
model has a specific set of parameters, the base class
\code{LearningApplicationBase} knows how to expose each type of classifier with
its dedicated parameters (a task that is a bit tedious so we want to implement
it only once). The \code{DoInit()} method creates a choice parameter named
\code{classifier} which contains the different supported classifiers along
with their parameters.

The function \code{Train(...)} provide an easy way to train the selected
classifier, with the corresponding parameters, and save the model to file.

On the other hand, the function \code{Classify(...)} allows to load a model
from file and apply it on a list of samples.

\section{Unsupervised classification}

\subsection{K-Means Classification}
\label{sec:KMeansClassifier}

\subsubsection{Shark version}

The KMeans algorithm has been implemented in Shark library, and has been
wrapped in the OTB machine learning framework. It is the first unsupervised
algorithm in this framework. It can be used in the same way as other machine
learning models. Remember that even if unsupervised model don't use a label
information on the samples, the target ListSample still has to be set in
\code{MachineLearningModel}. A ListSample filled with zeros can be used.

This model uses a hard clustering model with the following parameters:
\begin{itemize}
\item The maximum number of iterations
\item The number of centroids (K)
\item An option to normalize input samples
\end{itemize}

As with Shark Random Forests, the training step is parallel.

\subsection{Kohonen's Self Organizing Map}
\label{sec:SOM}
\input{Kohonen}
%%%1. Construction SOM
\subsubsection{Building a color table}
\label{sec:SOMColorTable}
\input{SOMExample}
\subsubsection{SOM Classification}
\label{sec:SOMClassification}
\input{SOMClassifierExample}

\subsubsection{Multi-band, streamed classification}

\ifitkFullVersion
\input{SOMImageClassificationExample.tex}
\fi

%-------------------------------------------------------------------------------
\subsection{Statistical Segmentations}
\label{sec:StatisticalSegmentations}

%\subsection{Markov Random Fields}

\subsubsection{Stochastic Expectation Maximization}
\label{sec:SEM}

The Stochastic Expectation Maximization (SEM) approach is a stochastic 
version of the EM mixture estimation seen on
section~\ref{sec:ExpectationMaximizationMixtureModelEstimation}. It has been 
introduced by \cite{CeDi95} to prevent convergence of the EM approach from
local minima. It avoids the analytical maximization issued by integrating a
stochastic sampling procedure in the estimation process. It induces an almost
sure (a.s.) convergence to the algorithm.

From the initial two step formulation of the EM mixture estimation, the SEM
may be decomposed into 3 steps:
\begin{enumerate}
\item \textbf{E-step}, calculates the expected membership values for each 
measurement vector to each classes.
\item \textbf{S-step}, performs a stochastic sampling of the membership vector
to each classes, according to the membership values computed in the E-step.
\item \textbf{M-step}, updates the parameters of the membership probabilities
(parameters to be defined through the class
\subdoxygen{itk}{Statistics}{ModelComponentBase} and its inherited classes).
\end{enumerate}
The implementation of the SEM has been turned to a contextual SEM in the sense
where the evaluation of the membership parameters is conditioned to
membership values of the spatial neighborhood of each pixels.

\ifitkFullVersion
\input{SEMModelEstimatorExample.tex}
\fi

%-------------------------------------------------------------------------------
\subsection{Classification using Markov Random Fields}
\label{sec:MarkovRandomField}

Markov Random Fields are probabilistic models that use the statistical
dependency between
pixels in a neighborhood to infeer the value of a give pixel.

\subsubsection{OTB framework}
\label{sec:MarkovRandomFieldOTB}
The ITK approach was considered not to be flexible enough for some
remote sensing applications. Therefore, we decided to implement our
own framework.
\index{Markov}

\begin{figure}[th]
  \centering
  \includegraphics[width=0.7\textwidth]{MarkovFramework.eps}
  \itkcaption[OTB Markov Framework]{OTB Markov Framework.}
  \label{fig:markovFramework}
\end{figure}

\index{Markov!Classification}
\ifitkFullVersion
\input{MarkovClassification1Example.tex}
\fi

\index{Markov!Classification}
\ifitkFullVersion
\input{MarkovClassification2Example.tex}
\fi

\index{Markov!Classification}
\ifitkFullVersion
\input{MarkovClassification3Example.tex}
\fi

\index{Markov!Regularization}
\ifitkFullVersion
\input{MarkovRegularizationExample.tex}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fusion of Classification maps}

%-------------------------------------------------------------------------------
\subsection{Dempster Shafer}

\subsubsection{General description}
A more adaptive fusion method using the Dempster Shafer theory 
(\href{http://en.wikipedia.org/wiki/Dempster-Shafer_theory}{http://en.wikipedia.org/wiki/Dempster-Shafer\_theory}) 
is available within the OTB. This method is adaptive as it is based on the 
so-called belief function of each class label for each classification map. Thus, 
each classified pixel is associated to a degree of confidence according to the 
classifier used. In the Dempster Shafer framework, the expert's point of view 
(i.e. with a high belief function) is considered as the truth. In order to 
estimate the belief function of each class label, we use the Dempster Shafer 
combination of masses of belief for each class label and for each classification 
map. In this framework, the output fused label of each pixel is the one with the 
maximal belief function.

Like for the majority voting method, the Dempster Shafer fusion handles not 
unique class labels with the maximal belief function. In this case, the output 
fused pixels are set to the undecided value.

The confidence levels of all the class labels are estimated from a comparison of 
the classification maps to fuse with a ground truth, which results in a 
confusion matrix. For each classification maps, these confusion matrices are then 
used to estimate the mass of belief of each class label.


\subsubsection{Mathematical formulation of the combination algorithm}

A description of the mathematical formulation of the Dempster Shafer combination 
algorithm is available in the following OTB Wiki page: 
\href{http://wiki.orfeo-toolbox.org/index.php/Information_fusion_framework}{http://wiki.orfeo-toolbox.org/index.php/Information\_fusion\_framework}.

\subsubsection{An example of Dempster Shafer fusion}
\ifitkFullVersion
\input{DempsterShaferFusionOfClassificationMapsExample.tex}
\fi


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classification map regularization}

%\subsection{Regularization by neighborhood-based majority voting}

\ifitkFullVersion
\input{ClassificationMapRegularizationExample.tex}
\fi



